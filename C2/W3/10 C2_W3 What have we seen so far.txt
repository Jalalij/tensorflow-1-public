What have we seen so far?


This week you've learned a lot of great new concepts!

You saw Transfer Learning, and how you can take an existing model, freeze many of its layers to prevent them being retrained, and effectively 'remember' the convolutions it was trained on to fit images. 

You then added your own DNN underneath this so that you could retrain on your images using the convolutions from the other model. 

You learned about regularization using dropout to make your network more efficient in preventing over-specialization and thus overfitting.

Before you get to the exercise for this week, let's have a quick quiz! 